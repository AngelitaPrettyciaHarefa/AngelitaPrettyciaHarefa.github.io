{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "j73mwRGP57xZ",
        "snJdzdFe5z5H",
        "AMmOwmwa_epJ",
        "KW_IOSBVenyG",
        "RWMHSAceARFG",
        "cHjGH4rJiVzK",
        "wNjqHbO9gFF1"
      ],
      "authorship_tag": "ABX9TyOZtEGZ+K1JO8WSmflu16cu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelitaPrettyciaHarefa/AngelitaPrettyciaHarefa.github.io/blob/main/SISTEM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCRAPING DATA"
      ],
      "metadata": {
        "id": "j73mwRGP57xZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OLuNF3M4BUL"
      },
      "outputs": [],
      "source": [
        "!pip install -qq google-play-scraper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pygments import highlight\n",
        "from pygments.lexers import JsonLexer\n",
        "from pygments.formatters import TerminalFormatter\n",
        "from google_play_scraper import Sort, reviews, app"
      ],
      "metadata": {
        "id": "h4qweNtv4Hbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gDrive/', force_remount=True)\n",
        "path = \"gDrive/MyDrive/\""
      ],
      "metadata": {
        "id": "AjTC1Bns4HeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_packages = [\n",
        "    'com.simasmobile.co.id'\n",
        "]"
      ],
      "metadata": {
        "id": "k2fm6zie4Hg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_json(json_object):\n",
        "  json_str = json.dumps(\n",
        "    json_object,\n",
        "    indent=2,\n",
        "    sort_keys=True,\n",
        "    default=str\n",
        "  )\n",
        "  print(highlight(json_str, JsonLexer(), TerminalFormatter()))"
      ],
      "metadata": {
        "id": "0sPg8q1H5abK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_reviews = []\n",
        "\n",
        "for ap in tqdm(app_packages):\n",
        "  rvs, continuation_token = reviews(\n",
        "      ap,\n",
        "      lang = 'id',\n",
        "      country = 'id',\n",
        "      sort = Sort.MOST_RELEVANT,\n",
        "      count=1000,\n",
        "      filter_score_with = None\n",
        "      )\n",
        "  app_reviews.extend(rvs)\n",
        "  rvs, _ = reviews(\n",
        "      ap,\n",
        "      continuation_token=continuation_token\n",
        "  )\n",
        "\n",
        "print_json(app_reviews[0])"
      ],
      "metadata": {
        "id": "NGXiGDwr4Hji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_app_reviews = pd.DataFrame(app_reviews)\n",
        "df_app_reviews.head()"
      ],
      "metadata": {
        "id": "Sb7sYMes4Ho6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_app_reviews = df_app_reviews.sort_values(by=\"at\", ascending=False)\n",
        "df_app_reviews.head()"
      ],
      "metadata": {
        "id": "BImdSGTO4HuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(app_reviews)"
      ],
      "metadata": {
        "id": "KfhYMCkP4HmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_app_reviews.info()"
      ],
      "metadata": {
        "id": "JeN29GUb4mqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_app_reviews.to_csv(path+'Reviews.csv', index=None, header=True)"
      ],
      "metadata": {
        "id": "hIOaX-Zo4nUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRE-PROCESSING DATA"
      ],
      "metadata": {
        "id": "snJdzdFe5z5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "id": "xHVK5PDh4nW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_app_reviews = pd.read_csv(path+\"Reviews.csv\")\n",
        "df_review = df_app_reviews[['content', 'score']]\n",
        "\n",
        "df_review.head()"
      ],
      "metadata": {
        "id": "wruzywpp4nZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== CLEANING ===============\n",
        "\n",
        "def remove_content_special(text):\n",
        "    # remove tab, new line, ans back slice\n",
        "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
        "    # remove non ASCII (emoticon, chinese word, .etc)\n",
        "    text = text.encode('ascii', 'replace').decode('ascii')\n",
        "    # remove mention, link, hashtag\n",
        "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "    # remove incomplete URL\n",
        "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
        "df_review['content_cleaning'] = df_review['content'].apply(remove_content_special)\n",
        "\n",
        "#remove number\n",
        "def remove_number(text):\n",
        "    return  re.sub(r\"\\d+\", \"\", text)\n",
        "df_review['content_cleaning'] = df_review['content_cleaning'].apply(remove_number)\n",
        "\n",
        "#remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "df_review['content_cleaning'] = df_review['content_cleaning'].apply(remove_punctuation)\n",
        "\n",
        "#remove whitespace leading & trailing\n",
        "def remove_whitespace_LT(text):\n",
        "    return text.strip()\n",
        "df_review['content_cleaning'] = df_review['content_cleaning'].apply(remove_whitespace_LT)\n",
        "\n",
        "#remove multiple whitespace into single whitespace\n",
        "def remove_whitespace_multiple(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "df_review['content_cleaning'] = df_review['content_cleaning'].apply(remove_whitespace_multiple)\n",
        "\n",
        "#remove single char\n",
        "def remove_singl_char(text):\n",
        "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
        "df_review['content_cleaning'] = df_review['content_cleaning'].apply(remove_singl_char)\n",
        "\n",
        "df_review.head()"
      ],
      "metadata": {
        "id": "T0xlIg294ncV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== CASE FOLDING ===============\n",
        "\n",
        "def case_folding(text):\n",
        "  text = text.lower()\n",
        "  return text\n",
        "df_review['content_case_folding'] = df_review['content_cleaning'].apply(case_folding)\n",
        "\n",
        "df_review.head()"
      ],
      "metadata": {
        "id": "8N4ENL7U4njF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== TOKENIZING ===============\n",
        "\n",
        "def word_tokenize_wrapper(text):\n",
        "    return word_tokenize(text)\n",
        "df_review['content_tokens'] = df_review['content_case_folding'].apply(word_tokenize_wrapper)\n",
        "\n",
        "df_review.head()"
      ],
      "metadata": {
        "id": "FsSZB7nn4nmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================ STOPWORDS REMOVAL ===============\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words('indonesian')\n",
        "list_stopwords = ['sinarmas', 'yg', 'gak', 'gk', 'dmn', 'nya', 'tp', 'bgs', 'dgn', 'bkn']\n",
        "stopwords.extend(list_stopwords)\n",
        "\n",
        "print(stopwords)"
      ],
      "metadata": {
        "id": "_8ZZBLAs4npA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words('indonesian')\n",
        "list_stopwords = ['sinarmas', 'yg', 'gak', 'gk', 'dmn', 'nya', 'tp', 'bgs', 'dgn', 'bkn']\n",
        "stopwords.extend(list_stopwords)\n",
        "\n",
        "#remove stopword pada list token\n",
        "def stopwords_removal(words):\n",
        "    return [word for word in words if word not in stopwords]\n",
        "df_review['content_stopwords_removal'] = df_review['content_tokens'].apply(stopwords_removal)\n",
        "\n",
        "df_review.head()"
      ],
      "metadata": {
        "id": "SO_L3CdQ4nr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== STEMMING ===============\n",
        "\n",
        "!pip install Sastrawi\n",
        "!pip install swifter\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import swifter"
      ],
      "metadata": {
        "id": "Uc4rbqPH4nuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Membuat fungsi untuk stemming\n",
        "def stemming(text):\n",
        "  text = [stemmer.stem(word) for word in text]\n",
        "  return text\n",
        "df_review['content_stemmed'] = df_review['content_stopwords_removal'].swifter.apply(stemming)\n",
        "\n",
        "df_review.head()"
      ],
      "metadata": {
        "id": "MO_yAMXq4nxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_review['content_preprocessing'] = df_review['content_stemmed'].apply(' '.join)\n",
        "df_review.head()"
      ],
      "metadata": {
        "id": "PTzE1jOMQz6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_review['content_preprocessing'] = df_review['content_preprocessing'].astype('str')\n",
        "\n",
        "df_review.info()"
      ],
      "metadata": {
        "id": "8qDoDrTeNTGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_review.to_csv(path+\"Text_Preprocessing.csv\", index=None, header=True)"
      ],
      "metadata": {
        "id": "_8mdgf408gEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PELABELAN (SENTIMEN)"
      ],
      "metadata": {
        "id": "AMmOwmwa_epJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_preprocessing = pd.read_csv(path+\"Text_Preprocessing.csv\")\n",
        "\n",
        "df_preprocessing.head()"
      ],
      "metadata": {
        "id": "kKEWE0R6_jvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preprocessing['score'].value_counts()"
      ],
      "metadata": {
        "id": "C3YNnya0Cvap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== PELABELAN ===============\n",
        "label = []\n",
        "\n",
        "for i, row in df_preprocessing.iterrows():\n",
        "  if row['score'] > 3:\n",
        "    label.append(1)\n",
        "  elif row['score'] < 3:\n",
        "    label.append(-1)\n",
        "  else:\n",
        "    label.append(0)\n",
        "\n",
        "df_preprocessing['label'] = label\n",
        "\n",
        "df_preprocessing.head()"
      ],
      "metadata": {
        "id": "SWnwR7H_Cvdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preprocessing['label'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "lMjeVOqLCvgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preprocessing.info()"
      ],
      "metadata": {
        "id": "0-483kPxCvjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============== WORDCLOUD ===============\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Xzplesi6Cvpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Polarity == 1 Positif\n",
        "train_positif = df_preprocessing[df_preprocessing['label'] == 1]\n",
        "all_text_positif = ' '.join(map(str, [word for word in train_positif['content_join']]))\n",
        "wordcloud = WordCloud(colormap='Blues', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_positif)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title('Visualisasi Sentimen Positif')\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9fIJVwvCvsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Polarity == 0 Netral\n",
        "train_netral = df_preprocessing[df_preprocessing[\"label\"] == 0]\n",
        "all_text_netral = ' '.join(map(str, [word for word in train_netral[\"content_join\"]]))\n",
        "wordcloud = WordCloud(colormap='Greens', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_netral)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title('Visualisasi Sentimen Netral')\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4cWWdEw4Cvvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Polarity == -1 Negatif\n",
        "train_negatif = df_preprocessing[df_preprocessing[\"label\"] == -1]\n",
        "all_text_negatif = ' '.join(map(str, [word for word in train_negatif[\"content_join\"]]))\n",
        "wordcloud = WordCloud(colormap='Reds', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_negatif)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title('Visualisasi Sentimen Negatif')\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.margins(x=0, y=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i2KHDvHfCvyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "#Shape of the dataset, and breakdown of the classes\n",
        "print(f\"rating 1 = {len(df_preprocessing[df_preprocessing['score']==1])} rows\")\n",
        "print(f\"rating 2 = {len(df_preprocessing[df_preprocessing['score']==2])} rows\")\n",
        "print(f\"rating 3 = {len(df_preprocessing[df_preprocessing['score']==3])} rows\")\n",
        "print(f\"rating 4 = {len(df_preprocessing[df_preprocessing['score']==4])} rows\")\n",
        "print(f\"rating 5 = {len(df_preprocessing[df_preprocessing['score']==5])} rows\")\n",
        "\n",
        "# Missing values in the dataset\n",
        "print(f\"Number of null in label: { df_preprocessing['score'].isnull().sum() }\")\n",
        "print(f\"Number of null in text: { df_preprocessing['score'].isnull().sum()}\")"
      ],
      "metadata": {
        "id": "q9V6ueQIochc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='score', data=df_preprocessing);"
      ],
      "metadata": {
        "id": "ZdT_3r4God6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "#Shape of the dataset, and breakdown of the classes\n",
        "print(f\"1 = {len(df_preprocessing[df_preprocessing['label']==1])} rows\")\n",
        "print(f\"0 = {len(df_preprocessing[df_preprocessing['label']==0])} rows\")\n",
        "print(f\"-1 = {len(df_preprocessing[df_preprocessing['label']==-1])} rows\")\n",
        "\n",
        "# Missing values in the dataset\n",
        "print(f\"Number of null in label: { df_preprocessing['label'].isnull().sum() }\")\n",
        "print(f\"Number of null in text: { df_preprocessing['label'].isnull().sum()}\")"
      ],
      "metadata": {
        "id": "BBBitJkR1LTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Rating Score Ulasan')\n",
        "sns.countplot(x='label', data=df_preprocessing);"
      ],
      "metadata": {
        "id": "ak53OUQ21b5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = df_preprocessing['score'].value_counts()\n",
        "\n",
        "# Membuat plot bar chart\n",
        "plt.figure(figsize=(6, 6))\n",
        "data.plot(kind='bar')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Jumlah Ulasan')\n",
        "plt.title('Rating Score Ulasan')\n",
        "\n",
        "# Menampilkan plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2dcYlKB68Crq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Membuat plot bar chart\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "plt.axes().bar(df_preprocessing['label'], df_preprocessing['score'])\n",
        "\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Rating Score Ulasan')\n",
        "\n",
        "# Menampilkan plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u_-e9NIF-4NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = df_preprocessing['score'].value_counts()\n",
        "explode = [0, 0, 0, 0, 0]\n",
        "\n",
        "# Membuat plot pie chart\n",
        "plt.figure(figsize=(7, 7))\n",
        "data.plot(kind='pie', autopct='%1.1f%%', startangle=90, explode=explode, label='')\n",
        "\n",
        "# Menampilkan plot\n",
        "plt.title('Rating Score Ulasan')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oKOnOnNHxMb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = df_preprocessing['label'].value_counts()\n",
        "explode = [0, 0, 0]\n",
        "\n",
        "# Membuat plot pie chart\n",
        "plt.figure(figsize=(7, 7))\n",
        "data.plot(kind='pie', autopct='%1.1f%%', startangle=90, explode=explode, label='')\n",
        "\n",
        "# Menampilkan plot\n",
        "plt.title('Sentimen Score Ulasan')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UtR3ohOI0Ocm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preprocessing.to_csv(path+\"Pelabelan_Text.csv\", index=None, header=True)"
      ],
      "metadata": {
        "id": "WsO455IjC9RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "KW_IOSBVenyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "text_tf = vectorizer.fit_transform(df_preprocessing['content_join'].astype('U'))\n",
        "print(text_tf)"
      ],
      "metadata": {
        "id": "cwz_FLr4yyAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "qXiwpOSLyyC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tf.todense()"
      ],
      "metadata": {
        "id": "AA8c_kiwyyFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_tf.todense().shape)"
      ],
      "metadata": {
        "id": "MJTbA0pRyyKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(text_tf.todense().T,\n",
        "                  index=vectorizer.get_feature_names_out(),\n",
        "                  columns=[f'D{i+1}' for i in range(len(df_preprocessing['content_join']))])\n",
        "df"
      ],
      "metadata": {
        "id": "hXetulTwyyNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PEMBAGIAN DATA TRAINING DAN DATA TESTING\n",
        "80:20"
      ],
      "metadata": {
        "id": "RWMHSAceARFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SPLITING DATA coba k fold close validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = text_tf\n",
        "y = df_preprocessing['label']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "cwjtTGVCAVBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OVERSAMPLING\n",
        "print(\"Before Positif : {}\" .format(sum(y_train == 1)))\n",
        "print(\"Before Negatif : {}\" .format(sum(y_train == -1)))\n",
        "print(\"Before Netral  : {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state = 42)\n",
        "x_train_res, y_train_res = sm.fit_resample(x_train, y_train.ravel())\n",
        "\n",
        "print(\"After x_train : {}\".format(x_train_res.shape))\n",
        "print(\"After y_train : {} \\n\".format(y_train_res.shape))\n",
        "\n",
        "print(\"After Positif : {}\".format(sum(y_train_res == 1)))\n",
        "print(\"After Negatif : {}\".format(sum(y_train_res == -1)))\n",
        "print(\"After Netral  : {}\".format(sum(y_train_res == 0)))"
      ],
      "metadata": {
        "id": "XrHgElqLAWZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPLEMENTASI METODE SVM"
      ],
      "metadata": {
        "id": "cHjGH4rJiVzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "elf = svm.SVC(kernel='linear').fit(x_train_res, y_train_res)\n",
        "predicted = elf.predict(x_test)\n",
        "print(\"SVM Accuracy \", accuracy_score(y_test, predicted))\n",
        "print(\"SVM Precision \", precision_score(y_test, predicted, average=\"macro\", pos_label=\"-1\"))\n",
        "print(\"SVM Recall \", recall_score(y_test, predicted, average=\"macro\", pos_label=\"-1\"))\n",
        "print(\"SVM f1_Score \", f1_score(y_test, predicted, average=\"macro\", pos_label=\"-1\"))\n",
        "\n",
        "print(f'confusion Matrix : \\n {confusion_matrix(y_test, predicted)}')\n",
        "\n",
        "print(classification_report(y_test, predicted, zero_division=0))"
      ],
      "metadata": {
        "id": "KNFVBf2K-hy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Data aktual\n",
        "y_actual = [\"positif\", \"negatif\", \"netral\", \"positif\", \"netral\", \"negatif\", \"positif\", \"netral\"]\n",
        "\n",
        "# Data prediksi\n",
        "y_pred = [\"positif\", \"negatif\", \"netral\", \"positif\", \"positif\", \"negatif\", \"netral\", \"negatif\"]\n",
        "\n",
        "# Definisikan label sentimen\n",
        "labels = [\"positif\", \"negatif\", \"netral\"]\n",
        "\n",
        "# Hitung confusion matrix\n",
        "cm = confusion_matrix(y_actual, y_pred, labels=labels)\n",
        "\n",
        "# Visualisasikan confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g7uas05y-nlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST\n",
        "Irwansight"
      ],
      "metadata": {
        "id": "wNjqHbO9gFF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#y = score"
      ],
      "metadata": {
        "id": "IOYD0Wm_ghxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling imbalanced\n",
        "#from imblearn.over_sampling import SMOTE\n",
        "#smote = SMOTE(k_neighbors = 1)\n",
        "#x_sm, y_sm = smote.fit_resample(X_tfidf,y)"
      ],
      "metadata": {
        "id": "IWFtzStpgh0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn import model_selection\n",
        "#Train_X, Test_X, Train_Y, Test_Y =model_selection.train_test_split(x_sm, y_sm, test_size = 0.1, random_state = 0)"
      ],
      "metadata": {
        "id": "6E9x9cY3gh3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.metrics import f1_score\n",
        "#from sklearn.svm import SVC\n",
        "# clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
        "#svm = SVC()\n",
        "#svm.fit(Train_X, Train_Y)\n",
        "#Pred_Y = svm.predict(Test_X)"
      ],
      "metadata": {
        "id": "Us2047VJgh6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#f1_score(Test_Y, Pred_Y, average='macro')"
      ],
      "metadata": {
        "id": "IgzecBp4gh83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#f1_score(Test_Y, Pred_Y, average='micro')"
      ],
      "metadata": {
        "id": "DC6bN70_giLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#f1_score(Test_Y, Pred_Y, average='weighted')"
      ],
      "metadata": {
        "id": "mh_WeIzThRRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#f1_score(Test_Y, Pred_Y, average=None)"
      ],
      "metadata": {
        "id": "1J14NRxdhRUV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}